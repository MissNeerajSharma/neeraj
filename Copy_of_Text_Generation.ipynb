{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Text Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NEERAJSHARMABSDU/neeraj/blob/master/Copy_of_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEWUJj_ELTzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWvGYrz6LTzY",
        "colab_type": "code",
        "outputId": "b772f171-ff4a-475e-aaa9-068654dc267f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load text\n",
        "raw_text = load_doc('shakespeare.txt')\n",
        "print(raw_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou contracted to thine own bright eyes,\n",
            "Feed'st thy light's flame with self-substantial fuel,\n",
            "Making a famine where abundance lies,\n",
            "Thy self thy foe, to thy sweet self too cruel:\n",
            "Thou that art now the world's fresh ornament,\n",
            "And only herald to the gaudy spring,\n",
            "Within thine own bud buriest thy content,\n",
            "And tender churl mak'st waste in niggarding:\n",
            "Pity the world, or else this glutton be,\n",
            "To eat the world's due, by the grave and thee.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDZyPRHTLTze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l96TV7aMLTzi",
        "colab_type": "code",
        "outputId": "9dab88e3-9b5e-458c-e49c-4cce9c8835b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10046
        }
      },
      "source": [
        "#organize into sequences of characters\n",
        "length = 12\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "    # select sequence of tokens\n",
        "    seq = raw_text[i-length:i+1]\n",
        "    # store\n",
        "    sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "\n",
        "#Total Sequences: 597\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()    \n",
        "# save sequences to file\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 1\n",
            "Total Sequences: 2\n",
            "Total Sequences: 3\n",
            "Total Sequences: 4\n",
            "Total Sequences: 5\n",
            "Total Sequences: 6\n",
            "Total Sequences: 7\n",
            "Total Sequences: 8\n",
            "Total Sequences: 9\n",
            "Total Sequences: 10\n",
            "Total Sequences: 11\n",
            "Total Sequences: 12\n",
            "Total Sequences: 13\n",
            "Total Sequences: 14\n",
            "Total Sequences: 15\n",
            "Total Sequences: 16\n",
            "Total Sequences: 17\n",
            "Total Sequences: 18\n",
            "Total Sequences: 19\n",
            "Total Sequences: 20\n",
            "Total Sequences: 21\n",
            "Total Sequences: 22\n",
            "Total Sequences: 23\n",
            "Total Sequences: 24\n",
            "Total Sequences: 25\n",
            "Total Sequences: 26\n",
            "Total Sequences: 27\n",
            "Total Sequences: 28\n",
            "Total Sequences: 29\n",
            "Total Sequences: 30\n",
            "Total Sequences: 31\n",
            "Total Sequences: 32\n",
            "Total Sequences: 33\n",
            "Total Sequences: 34\n",
            "Total Sequences: 35\n",
            "Total Sequences: 36\n",
            "Total Sequences: 37\n",
            "Total Sequences: 38\n",
            "Total Sequences: 39\n",
            "Total Sequences: 40\n",
            "Total Sequences: 41\n",
            "Total Sequences: 42\n",
            "Total Sequences: 43\n",
            "Total Sequences: 44\n",
            "Total Sequences: 45\n",
            "Total Sequences: 46\n",
            "Total Sequences: 47\n",
            "Total Sequences: 48\n",
            "Total Sequences: 49\n",
            "Total Sequences: 50\n",
            "Total Sequences: 51\n",
            "Total Sequences: 52\n",
            "Total Sequences: 53\n",
            "Total Sequences: 54\n",
            "Total Sequences: 55\n",
            "Total Sequences: 56\n",
            "Total Sequences: 57\n",
            "Total Sequences: 58\n",
            "Total Sequences: 59\n",
            "Total Sequences: 60\n",
            "Total Sequences: 61\n",
            "Total Sequences: 62\n",
            "Total Sequences: 63\n",
            "Total Sequences: 64\n",
            "Total Sequences: 65\n",
            "Total Sequences: 66\n",
            "Total Sequences: 67\n",
            "Total Sequences: 68\n",
            "Total Sequences: 69\n",
            "Total Sequences: 70\n",
            "Total Sequences: 71\n",
            "Total Sequences: 72\n",
            "Total Sequences: 73\n",
            "Total Sequences: 74\n",
            "Total Sequences: 75\n",
            "Total Sequences: 76\n",
            "Total Sequences: 77\n",
            "Total Sequences: 78\n",
            "Total Sequences: 79\n",
            "Total Sequences: 80\n",
            "Total Sequences: 81\n",
            "Total Sequences: 82\n",
            "Total Sequences: 83\n",
            "Total Sequences: 84\n",
            "Total Sequences: 85\n",
            "Total Sequences: 86\n",
            "Total Sequences: 87\n",
            "Total Sequences: 88\n",
            "Total Sequences: 89\n",
            "Total Sequences: 90\n",
            "Total Sequences: 91\n",
            "Total Sequences: 92\n",
            "Total Sequences: 93\n",
            "Total Sequences: 94\n",
            "Total Sequences: 95\n",
            "Total Sequences: 96\n",
            "Total Sequences: 97\n",
            "Total Sequences: 98\n",
            "Total Sequences: 99\n",
            "Total Sequences: 100\n",
            "Total Sequences: 101\n",
            "Total Sequences: 102\n",
            "Total Sequences: 103\n",
            "Total Sequences: 104\n",
            "Total Sequences: 105\n",
            "Total Sequences: 106\n",
            "Total Sequences: 107\n",
            "Total Sequences: 108\n",
            "Total Sequences: 109\n",
            "Total Sequences: 110\n",
            "Total Sequences: 111\n",
            "Total Sequences: 112\n",
            "Total Sequences: 113\n",
            "Total Sequences: 114\n",
            "Total Sequences: 115\n",
            "Total Sequences: 116\n",
            "Total Sequences: 117\n",
            "Total Sequences: 118\n",
            "Total Sequences: 119\n",
            "Total Sequences: 120\n",
            "Total Sequences: 121\n",
            "Total Sequences: 122\n",
            "Total Sequences: 123\n",
            "Total Sequences: 124\n",
            "Total Sequences: 125\n",
            "Total Sequences: 126\n",
            "Total Sequences: 127\n",
            "Total Sequences: 128\n",
            "Total Sequences: 129\n",
            "Total Sequences: 130\n",
            "Total Sequences: 131\n",
            "Total Sequences: 132\n",
            "Total Sequences: 133\n",
            "Total Sequences: 134\n",
            "Total Sequences: 135\n",
            "Total Sequences: 136\n",
            "Total Sequences: 137\n",
            "Total Sequences: 138\n",
            "Total Sequences: 139\n",
            "Total Sequences: 140\n",
            "Total Sequences: 141\n",
            "Total Sequences: 142\n",
            "Total Sequences: 143\n",
            "Total Sequences: 144\n",
            "Total Sequences: 145\n",
            "Total Sequences: 146\n",
            "Total Sequences: 147\n",
            "Total Sequences: 148\n",
            "Total Sequences: 149\n",
            "Total Sequences: 150\n",
            "Total Sequences: 151\n",
            "Total Sequences: 152\n",
            "Total Sequences: 153\n",
            "Total Sequences: 154\n",
            "Total Sequences: 155\n",
            "Total Sequences: 156\n",
            "Total Sequences: 157\n",
            "Total Sequences: 158\n",
            "Total Sequences: 159\n",
            "Total Sequences: 160\n",
            "Total Sequences: 161\n",
            "Total Sequences: 162\n",
            "Total Sequences: 163\n",
            "Total Sequences: 164\n",
            "Total Sequences: 165\n",
            "Total Sequences: 166\n",
            "Total Sequences: 167\n",
            "Total Sequences: 168\n",
            "Total Sequences: 169\n",
            "Total Sequences: 170\n",
            "Total Sequences: 171\n",
            "Total Sequences: 172\n",
            "Total Sequences: 173\n",
            "Total Sequences: 174\n",
            "Total Sequences: 175\n",
            "Total Sequences: 176\n",
            "Total Sequences: 177\n",
            "Total Sequences: 178\n",
            "Total Sequences: 179\n",
            "Total Sequences: 180\n",
            "Total Sequences: 181\n",
            "Total Sequences: 182\n",
            "Total Sequences: 183\n",
            "Total Sequences: 184\n",
            "Total Sequences: 185\n",
            "Total Sequences: 186\n",
            "Total Sequences: 187\n",
            "Total Sequences: 188\n",
            "Total Sequences: 189\n",
            "Total Sequences: 190\n",
            "Total Sequences: 191\n",
            "Total Sequences: 192\n",
            "Total Sequences: 193\n",
            "Total Sequences: 194\n",
            "Total Sequences: 195\n",
            "Total Sequences: 196\n",
            "Total Sequences: 197\n",
            "Total Sequences: 198\n",
            "Total Sequences: 199\n",
            "Total Sequences: 200\n",
            "Total Sequences: 201\n",
            "Total Sequences: 202\n",
            "Total Sequences: 203\n",
            "Total Sequences: 204\n",
            "Total Sequences: 205\n",
            "Total Sequences: 206\n",
            "Total Sequences: 207\n",
            "Total Sequences: 208\n",
            "Total Sequences: 209\n",
            "Total Sequences: 210\n",
            "Total Sequences: 211\n",
            "Total Sequences: 212\n",
            "Total Sequences: 213\n",
            "Total Sequences: 214\n",
            "Total Sequences: 215\n",
            "Total Sequences: 216\n",
            "Total Sequences: 217\n",
            "Total Sequences: 218\n",
            "Total Sequences: 219\n",
            "Total Sequences: 220\n",
            "Total Sequences: 221\n",
            "Total Sequences: 222\n",
            "Total Sequences: 223\n",
            "Total Sequences: 224\n",
            "Total Sequences: 225\n",
            "Total Sequences: 226\n",
            "Total Sequences: 227\n",
            "Total Sequences: 228\n",
            "Total Sequences: 229\n",
            "Total Sequences: 230\n",
            "Total Sequences: 231\n",
            "Total Sequences: 232\n",
            "Total Sequences: 233\n",
            "Total Sequences: 234\n",
            "Total Sequences: 235\n",
            "Total Sequences: 236\n",
            "Total Sequences: 237\n",
            "Total Sequences: 238\n",
            "Total Sequences: 239\n",
            "Total Sequences: 240\n",
            "Total Sequences: 241\n",
            "Total Sequences: 242\n",
            "Total Sequences: 243\n",
            "Total Sequences: 244\n",
            "Total Sequences: 245\n",
            "Total Sequences: 246\n",
            "Total Sequences: 247\n",
            "Total Sequences: 248\n",
            "Total Sequences: 249\n",
            "Total Sequences: 250\n",
            "Total Sequences: 251\n",
            "Total Sequences: 252\n",
            "Total Sequences: 253\n",
            "Total Sequences: 254\n",
            "Total Sequences: 255\n",
            "Total Sequences: 256\n",
            "Total Sequences: 257\n",
            "Total Sequences: 258\n",
            "Total Sequences: 259\n",
            "Total Sequences: 260\n",
            "Total Sequences: 261\n",
            "Total Sequences: 262\n",
            "Total Sequences: 263\n",
            "Total Sequences: 264\n",
            "Total Sequences: 265\n",
            "Total Sequences: 266\n",
            "Total Sequences: 267\n",
            "Total Sequences: 268\n",
            "Total Sequences: 269\n",
            "Total Sequences: 270\n",
            "Total Sequences: 271\n",
            "Total Sequences: 272\n",
            "Total Sequences: 273\n",
            "Total Sequences: 274\n",
            "Total Sequences: 275\n",
            "Total Sequences: 276\n",
            "Total Sequences: 277\n",
            "Total Sequences: 278\n",
            "Total Sequences: 279\n",
            "Total Sequences: 280\n",
            "Total Sequences: 281\n",
            "Total Sequences: 282\n",
            "Total Sequences: 283\n",
            "Total Sequences: 284\n",
            "Total Sequences: 285\n",
            "Total Sequences: 286\n",
            "Total Sequences: 287\n",
            "Total Sequences: 288\n",
            "Total Sequences: 289\n",
            "Total Sequences: 290\n",
            "Total Sequences: 291\n",
            "Total Sequences: 292\n",
            "Total Sequences: 293\n",
            "Total Sequences: 294\n",
            "Total Sequences: 295\n",
            "Total Sequences: 296\n",
            "Total Sequences: 297\n",
            "Total Sequences: 298\n",
            "Total Sequences: 299\n",
            "Total Sequences: 300\n",
            "Total Sequences: 301\n",
            "Total Sequences: 302\n",
            "Total Sequences: 303\n",
            "Total Sequences: 304\n",
            "Total Sequences: 305\n",
            "Total Sequences: 306\n",
            "Total Sequences: 307\n",
            "Total Sequences: 308\n",
            "Total Sequences: 309\n",
            "Total Sequences: 310\n",
            "Total Sequences: 311\n",
            "Total Sequences: 312\n",
            "Total Sequences: 313\n",
            "Total Sequences: 314\n",
            "Total Sequences: 315\n",
            "Total Sequences: 316\n",
            "Total Sequences: 317\n",
            "Total Sequences: 318\n",
            "Total Sequences: 319\n",
            "Total Sequences: 320\n",
            "Total Sequences: 321\n",
            "Total Sequences: 322\n",
            "Total Sequences: 323\n",
            "Total Sequences: 324\n",
            "Total Sequences: 325\n",
            "Total Sequences: 326\n",
            "Total Sequences: 327\n",
            "Total Sequences: 328\n",
            "Total Sequences: 329\n",
            "Total Sequences: 330\n",
            "Total Sequences: 331\n",
            "Total Sequences: 332\n",
            "Total Sequences: 333\n",
            "Total Sequences: 334\n",
            "Total Sequences: 335\n",
            "Total Sequences: 336\n",
            "Total Sequences: 337\n",
            "Total Sequences: 338\n",
            "Total Sequences: 339\n",
            "Total Sequences: 340\n",
            "Total Sequences: 341\n",
            "Total Sequences: 342\n",
            "Total Sequences: 343\n",
            "Total Sequences: 344\n",
            "Total Sequences: 345\n",
            "Total Sequences: 346\n",
            "Total Sequences: 347\n",
            "Total Sequences: 348\n",
            "Total Sequences: 349\n",
            "Total Sequences: 350\n",
            "Total Sequences: 351\n",
            "Total Sequences: 352\n",
            "Total Sequences: 353\n",
            "Total Sequences: 354\n",
            "Total Sequences: 355\n",
            "Total Sequences: 356\n",
            "Total Sequences: 357\n",
            "Total Sequences: 358\n",
            "Total Sequences: 359\n",
            "Total Sequences: 360\n",
            "Total Sequences: 361\n",
            "Total Sequences: 362\n",
            "Total Sequences: 363\n",
            "Total Sequences: 364\n",
            "Total Sequences: 365\n",
            "Total Sequences: 366\n",
            "Total Sequences: 367\n",
            "Total Sequences: 368\n",
            "Total Sequences: 369\n",
            "Total Sequences: 370\n",
            "Total Sequences: 371\n",
            "Total Sequences: 372\n",
            "Total Sequences: 373\n",
            "Total Sequences: 374\n",
            "Total Sequences: 375\n",
            "Total Sequences: 376\n",
            "Total Sequences: 377\n",
            "Total Sequences: 378\n",
            "Total Sequences: 379\n",
            "Total Sequences: 380\n",
            "Total Sequences: 381\n",
            "Total Sequences: 382\n",
            "Total Sequences: 383\n",
            "Total Sequences: 384\n",
            "Total Sequences: 385\n",
            "Total Sequences: 386\n",
            "Total Sequences: 387\n",
            "Total Sequences: 388\n",
            "Total Sequences: 389\n",
            "Total Sequences: 390\n",
            "Total Sequences: 391\n",
            "Total Sequences: 392\n",
            "Total Sequences: 393\n",
            "Total Sequences: 394\n",
            "Total Sequences: 395\n",
            "Total Sequences: 396\n",
            "Total Sequences: 397\n",
            "Total Sequences: 398\n",
            "Total Sequences: 399\n",
            "Total Sequences: 400\n",
            "Total Sequences: 401\n",
            "Total Sequences: 402\n",
            "Total Sequences: 403\n",
            "Total Sequences: 404\n",
            "Total Sequences: 405\n",
            "Total Sequences: 406\n",
            "Total Sequences: 407\n",
            "Total Sequences: 408\n",
            "Total Sequences: 409\n",
            "Total Sequences: 410\n",
            "Total Sequences: 411\n",
            "Total Sequences: 412\n",
            "Total Sequences: 413\n",
            "Total Sequences: 414\n",
            "Total Sequences: 415\n",
            "Total Sequences: 416\n",
            "Total Sequences: 417\n",
            "Total Sequences: 418\n",
            "Total Sequences: 419\n",
            "Total Sequences: 420\n",
            "Total Sequences: 421\n",
            "Total Sequences: 422\n",
            "Total Sequences: 423\n",
            "Total Sequences: 424\n",
            "Total Sequences: 425\n",
            "Total Sequences: 426\n",
            "Total Sequences: 427\n",
            "Total Sequences: 428\n",
            "Total Sequences: 429\n",
            "Total Sequences: 430\n",
            "Total Sequences: 431\n",
            "Total Sequences: 432\n",
            "Total Sequences: 433\n",
            "Total Sequences: 434\n",
            "Total Sequences: 435\n",
            "Total Sequences: 436\n",
            "Total Sequences: 437\n",
            "Total Sequences: 438\n",
            "Total Sequences: 439\n",
            "Total Sequences: 440\n",
            "Total Sequences: 441\n",
            "Total Sequences: 442\n",
            "Total Sequences: 443\n",
            "Total Sequences: 444\n",
            "Total Sequences: 445\n",
            "Total Sequences: 446\n",
            "Total Sequences: 447\n",
            "Total Sequences: 448\n",
            "Total Sequences: 449\n",
            "Total Sequences: 450\n",
            "Total Sequences: 451\n",
            "Total Sequences: 452\n",
            "Total Sequences: 453\n",
            "Total Sequences: 454\n",
            "Total Sequences: 455\n",
            "Total Sequences: 456\n",
            "Total Sequences: 457\n",
            "Total Sequences: 458\n",
            "Total Sequences: 459\n",
            "Total Sequences: 460\n",
            "Total Sequences: 461\n",
            "Total Sequences: 462\n",
            "Total Sequences: 463\n",
            "Total Sequences: 464\n",
            "Total Sequences: 465\n",
            "Total Sequences: 466\n",
            "Total Sequences: 467\n",
            "Total Sequences: 468\n",
            "Total Sequences: 469\n",
            "Total Sequences: 470\n",
            "Total Sequences: 471\n",
            "Total Sequences: 472\n",
            "Total Sequences: 473\n",
            "Total Sequences: 474\n",
            "Total Sequences: 475\n",
            "Total Sequences: 476\n",
            "Total Sequences: 477\n",
            "Total Sequences: 478\n",
            "Total Sequences: 479\n",
            "Total Sequences: 480\n",
            "Total Sequences: 481\n",
            "Total Sequences: 482\n",
            "Total Sequences: 483\n",
            "Total Sequences: 484\n",
            "Total Sequences: 485\n",
            "Total Sequences: 486\n",
            "Total Sequences: 487\n",
            "Total Sequences: 488\n",
            "Total Sequences: 489\n",
            "Total Sequences: 490\n",
            "Total Sequences: 491\n",
            "Total Sequences: 492\n",
            "Total Sequences: 493\n",
            "Total Sequences: 494\n",
            "Total Sequences: 495\n",
            "Total Sequences: 496\n",
            "Total Sequences: 497\n",
            "Total Sequences: 498\n",
            "Total Sequences: 499\n",
            "Total Sequences: 500\n",
            "Total Sequences: 501\n",
            "Total Sequences: 502\n",
            "Total Sequences: 503\n",
            "Total Sequences: 504\n",
            "Total Sequences: 505\n",
            "Total Sequences: 506\n",
            "Total Sequences: 507\n",
            "Total Sequences: 508\n",
            "Total Sequences: 509\n",
            "Total Sequences: 510\n",
            "Total Sequences: 511\n",
            "Total Sequences: 512\n",
            "Total Sequences: 513\n",
            "Total Sequences: 514\n",
            "Total Sequences: 515\n",
            "Total Sequences: 516\n",
            "Total Sequences: 517\n",
            "Total Sequences: 518\n",
            "Total Sequences: 519\n",
            "Total Sequences: 520\n",
            "Total Sequences: 521\n",
            "Total Sequences: 522\n",
            "Total Sequences: 523\n",
            "Total Sequences: 524\n",
            "Total Sequences: 525\n",
            "Total Sequences: 526\n",
            "Total Sequences: 527\n",
            "Total Sequences: 528\n",
            "Total Sequences: 529\n",
            "Total Sequences: 530\n",
            "Total Sequences: 531\n",
            "Total Sequences: 532\n",
            "Total Sequences: 533\n",
            "Total Sequences: 534\n",
            "Total Sequences: 535\n",
            "Total Sequences: 536\n",
            "Total Sequences: 537\n",
            "Total Sequences: 538\n",
            "Total Sequences: 539\n",
            "Total Sequences: 540\n",
            "Total Sequences: 541\n",
            "Total Sequences: 542\n",
            "Total Sequences: 543\n",
            "Total Sequences: 544\n",
            "Total Sequences: 545\n",
            "Total Sequences: 546\n",
            "Total Sequences: 547\n",
            "Total Sequences: 548\n",
            "Total Sequences: 549\n",
            "Total Sequences: 550\n",
            "Total Sequences: 551\n",
            "Total Sequences: 552\n",
            "Total Sequences: 553\n",
            "Total Sequences: 554\n",
            "Total Sequences: 555\n",
            "Total Sequences: 556\n",
            "Total Sequences: 557\n",
            "Total Sequences: 558\n",
            "Total Sequences: 559\n",
            "Total Sequences: 560\n",
            "Total Sequences: 561\n",
            "Total Sequences: 562\n",
            "Total Sequences: 563\n",
            "Total Sequences: 564\n",
            "Total Sequences: 565\n",
            "Total Sequences: 566\n",
            "Total Sequences: 567\n",
            "Total Sequences: 568\n",
            "Total Sequences: 569\n",
            "Total Sequences: 570\n",
            "Total Sequences: 571\n",
            "Total Sequences: 572\n",
            "Total Sequences: 573\n",
            "Total Sequences: 574\n",
            "Total Sequences: 575\n",
            "Total Sequences: 576\n",
            "Total Sequences: 577\n",
            "Total Sequences: 578\n",
            "Total Sequences: 579\n",
            "Total Sequences: 580\n",
            "Total Sequences: 581\n",
            "Total Sequences: 582\n",
            "Total Sequences: 583\n",
            "Total Sequences: 584\n",
            "Total Sequences: 585\n",
            "Total Sequences: 586\n",
            "Total Sequences: 587\n",
            "Total Sequences: 588\n",
            "Total Sequences: 589\n",
            "Total Sequences: 590\n",
            "Total Sequences: 591\n",
            "Total Sequences: 592\n",
            "Total Sequences: 593\n",
            "Total Sequences: 594\n",
            "Total Sequences: 595\n",
            "Total Sequences: 596\n",
            "Total Sequences: 597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3ycn9M5LTzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load\n",
        "in_filename = 'char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_gJ23RCLTzt",
        "colab_type": "code",
        "outputId": "f6bdfde7-9d7d-48ce-eeec-3b252c24193b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences = list()\n",
        "for line in lines:\n",
        "    #integer encode line\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    #store\n",
        "    sequences.append(encoded_seq)\n",
        "\n",
        "#vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEm_dj0mLTz0",
        "colab_type": "code",
        "outputId": "1a1cd78a-7815-480d-cf9f-32b9b61f102a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(597, 12, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6C0PkRlLTz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(X):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "   # plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q12ypzcLLTz-",
        "colab_type": "code",
        "outputId": "118880e2-9995-4f9b-9af3-673c378d100e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3699
        }
      },
      "source": [
        "# define model\n",
        "model = define_model(X)\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 75)                33900     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 37)                2812      \n",
            "=================================================================\n",
            "Total params: 36,712\n",
            "Trainable params: 36,712\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            " - 1s - loss: 3.5488 - acc: 0.1240\n",
            "Epoch 2/100\n",
            " - 0s - loss: 3.2005 - acc: 0.1742\n",
            "Epoch 3/100\n",
            " - 0s - loss: 3.0050 - acc: 0.1742\n",
            "Epoch 4/100\n",
            " - 0s - loss: 2.9713 - acc: 0.1742\n",
            "Epoch 5/100\n",
            " - 0s - loss: 2.9559 - acc: 0.1742\n",
            "Epoch 6/100\n",
            " - 0s - loss: 2.9461 - acc: 0.1742\n",
            "Epoch 7/100\n",
            " - 0s - loss: 2.9363 - acc: 0.1742\n",
            "Epoch 8/100\n",
            " - 0s - loss: 2.9322 - acc: 0.1742\n",
            "Epoch 9/100\n",
            " - 0s - loss: 2.9173 - acc: 0.1742\n",
            "Epoch 10/100\n",
            " - 0s - loss: 2.9025 - acc: 0.1742\n",
            "Epoch 11/100\n",
            " - 0s - loss: 2.8908 - acc: 0.1742\n",
            "Epoch 12/100\n",
            " - 0s - loss: 2.8756 - acc: 0.1742\n",
            "Epoch 13/100\n",
            " - 0s - loss: 2.8640 - acc: 0.1809\n",
            "Epoch 14/100\n",
            " - 0s - loss: 2.8528 - acc: 0.2127\n",
            "Epoch 15/100\n",
            " - 0s - loss: 2.8364 - acc: 0.1960\n",
            "Epoch 16/100\n",
            " - 0s - loss: 2.8106 - acc: 0.1876\n",
            "Epoch 17/100\n",
            " - 0s - loss: 2.7906 - acc: 0.1993\n",
            "Epoch 18/100\n",
            " - 0s - loss: 2.7544 - acc: 0.2211\n",
            "Epoch 19/100\n",
            " - 0s - loss: 2.7345 - acc: 0.2278\n",
            "Epoch 20/100\n",
            " - 0s - loss: 2.6866 - acc: 0.2446\n",
            "Epoch 21/100\n",
            " - 0s - loss: 2.6476 - acc: 0.2345\n",
            "Epoch 22/100\n",
            " - 0s - loss: 2.6175 - acc: 0.2596\n",
            "Epoch 23/100\n",
            " - 0s - loss: 2.5732 - acc: 0.2864\n",
            "Epoch 24/100\n",
            " - 0s - loss: 2.5291 - acc: 0.2864\n",
            "Epoch 25/100\n",
            " - 0s - loss: 2.4969 - acc: 0.3015\n",
            "Epoch 26/100\n",
            " - 0s - loss: 2.4617 - acc: 0.2948\n",
            "Epoch 27/100\n",
            " - 0s - loss: 2.4113 - acc: 0.3283\n",
            "Epoch 28/100\n",
            " - 0s - loss: 2.3611 - acc: 0.3166\n",
            "Epoch 29/100\n",
            " - 0s - loss: 2.3278 - acc: 0.3266\n",
            "Epoch 30/100\n",
            " - 0s - loss: 2.2768 - acc: 0.3417\n",
            "Epoch 31/100\n",
            " - 0s - loss: 2.2308 - acc: 0.3484\n",
            "Epoch 32/100\n",
            " - 0s - loss: 2.1853 - acc: 0.3668\n",
            "Epoch 33/100\n",
            " - 0s - loss: 2.1391 - acc: 0.3752\n",
            "Epoch 34/100\n",
            " - 0s - loss: 2.1376 - acc: 0.3819\n",
            "Epoch 35/100\n",
            " - 0s - loss: 2.1016 - acc: 0.3853\n",
            "Epoch 36/100\n",
            " - 0s - loss: 2.0330 - acc: 0.3953\n",
            "Epoch 37/100\n",
            " - 0s - loss: 2.0335 - acc: 0.4020\n",
            "Epoch 38/100\n",
            " - 0s - loss: 1.9868 - acc: 0.4305\n",
            "Epoch 39/100\n",
            " - 0s - loss: 1.9274 - acc: 0.4422\n",
            "Epoch 40/100\n",
            " - 0s - loss: 1.8732 - acc: 0.4573\n",
            "Epoch 41/100\n",
            " - 0s - loss: 1.8381 - acc: 0.4673\n",
            "Epoch 42/100\n",
            " - 0s - loss: 1.8143 - acc: 0.4690\n",
            "Epoch 43/100\n",
            " - 0s - loss: 1.8006 - acc: 0.4740\n",
            "Epoch 44/100\n",
            " - 0s - loss: 1.7679 - acc: 0.4975\n",
            "Epoch 45/100\n",
            " - 0s - loss: 1.7246 - acc: 0.5159\n",
            "Epoch 46/100\n",
            " - 0s - loss: 1.6810 - acc: 0.4992\n",
            "Epoch 47/100\n",
            " - 0s - loss: 1.6403 - acc: 0.5394\n",
            "Epoch 48/100\n",
            " - 0s - loss: 1.6009 - acc: 0.5427\n",
            "Epoch 49/100\n",
            " - 0s - loss: 1.5554 - acc: 0.5863\n",
            "Epoch 50/100\n",
            " - 0s - loss: 1.5152 - acc: 0.5678\n",
            "Epoch 51/100\n",
            " - 0s - loss: 1.4786 - acc: 0.6064\n",
            "Epoch 52/100\n",
            " - 0s - loss: 1.4392 - acc: 0.5980\n",
            "Epoch 53/100\n",
            " - 0s - loss: 1.3931 - acc: 0.6332\n",
            "Epoch 54/100\n",
            " - 0s - loss: 1.3595 - acc: 0.6248\n",
            "Epoch 55/100\n",
            " - 0s - loss: 1.3142 - acc: 0.6633\n",
            "Epoch 56/100\n",
            " - 0s - loss: 1.2867 - acc: 0.6717\n",
            "Epoch 57/100\n",
            " - 0s - loss: 1.2705 - acc: 0.6600\n",
            "Epoch 58/100\n",
            " - 0s - loss: 1.2028 - acc: 0.7002\n",
            "Epoch 59/100\n",
            " - 0s - loss: 1.1794 - acc: 0.6985\n",
            "Epoch 60/100\n",
            " - 0s - loss: 1.1241 - acc: 0.7303\n",
            "Epoch 61/100\n",
            " - 0s - loss: 1.0796 - acc: 0.7370\n",
            "Epoch 62/100\n",
            " - 0s - loss: 1.0533 - acc: 0.7571\n",
            "Epoch 63/100\n",
            " - 0s - loss: 1.0118 - acc: 0.7739\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.9949 - acc: 0.7755\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.9455 - acc: 0.7906\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.9087 - acc: 0.7940\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.8759 - acc: 0.8191\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.8386 - acc: 0.8241\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.8147 - acc: 0.8342\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.7643 - acc: 0.8509\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.7188 - acc: 0.8827\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.7091 - acc: 0.8827\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.6583 - acc: 0.8878\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.6316 - acc: 0.8995\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.6034 - acc: 0.9229\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.5758 - acc: 0.9229\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.5538 - acc: 0.9229\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.5146 - acc: 0.9481\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4952 - acc: 0.9481\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4713 - acc: 0.9481\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4436 - acc: 0.9631\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4241 - acc: 0.9715\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4114 - acc: 0.9665\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.3764 - acc: 0.9799\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.3542 - acc: 0.9782\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.3390 - acc: 0.9832\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.3222 - acc: 0.9849\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.3041 - acc: 0.9899\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.2871 - acc: 0.9883\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.2690 - acc: 0.9883\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.2530 - acc: 0.9899\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.2431 - acc: 0.9933\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.2388 - acc: 0.9916\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.2216 - acc: 0.9916\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.2087 - acc: 0.9916\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.1991 - acc: 0.9883\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.1900 - acc: 0.9933\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.1778 - acc: 0.9916\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.1716 - acc: 0.9933\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.1663 - acc: 0.9916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f917a8cc550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKf3HAZHLT0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il5zvep5LT0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# load the mapping\n",
        "mapping = load(open('mapping.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n31sDSLJLT0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    len(in_text)\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # one hot encode\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        encoded.shape\n",
        "        #encoded = encoded.reshape( 1,encoded.shape[0], encoded.shape[1])\n",
        "        # predict character\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # reverse map integer to character\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "            # append to input\n",
        "        in_text += out_char\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVa8awLbLT0S",
        "colab_type": "code",
        "outputId": "e3bd6984-90a9-4877-f73f-cd483d598395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# test start of rhyme\n",
        "print(generate_seq(model, mapping, 12, 'From fairest', 70))\n",
        "# test mid-line\n",
        "print(generate_seq(model, mapping, 12, 'Making a famine', 70))\n",
        "# test not in original\n",
        "print(generate_seq(model, mapping, 12, 'hello worl', 70))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase, That thereby beauty's rose might never \n",
            "Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: \n",
            "hello world bud sesttin tonef oo buryy oest to ty reee, The thearey bp tae ses m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoPphBr3LT0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}